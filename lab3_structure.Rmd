---
title: "Lab Assignment 3: Parameter Estimation and Unbiasedness of Estimators"
author: "Team [Team Number]"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Work Breakdown Structure

- Problem 1 - Lidiia Sokha
- Problem 2 - Nazar Pasichnyk
- Problem 3 - Viktoriia Kukurudza

**Total:** 100%

# Introduction

```{r seed}
# Set seed for reproducibility
set.seed(39)
```

---

# Part I: Parameter Estimation

## Problem 1: Exponential Distribution Confidence Intervals

### Problem Formulation and Discussion

**Objective:**

**Parameters:**
- θ = team_id/10
- α levels: 0.1, 0.05, 0.01
- Sample sizes: n (to be varied)
- Number of repetitions: m (to be varied)

**Discussion:**

### Method 1: Exact Distribution Using Chi-Square

#### Theoretical Background

#### R Implementation

```{r problem1-method1}
# Implementation for Method 1
```

#### Results and Statistics

```{r problem1-method1-results}
# Calculate statistics and create visualizations
```

### Method 2: Normal Approximation with Known Variance

#### Theoretical Background

#### R Implementation

```{r problem1-method2}
# Implementation for Method 2
```

#### Results and Statistics

```{r problem1-method2-results}
# Calculate statistics and create visualizations
```

### Method 3: Normal Approximation with Solved Inequality

#### Theoretical Background

#### R Implementation

```{r problem1-method3}
# Implementation for Method 3
```

#### Results and Statistics

```{r problem1-method3-results}
# Calculate statistics and create visualizations
```

### Method 4: Student t-Distribution Approach

#### Theoretical Background

#### R Implementation

```{r problem1-method4}
# Implementation for Method 4
```

#### Results and Statistics

```{r problem1-method4-results}
# Calculate statistics and create visualizations
```

### Task (a): Verification of Coverage Probability

```{r problem1-verification}
# Verify that confidence intervals contain θ approximately 100(1-α)% of times
```

#### Coverage Probability Results

#### Visualization

```{r problem1-coverage-plots}
# Histograms and plots showing coverage
```

### Task (b): Comparison of Precision (Interval Lengths)

```{r problem1-precision}
# Compare lengths of confidence intervals
```

#### Precision Comparison Results

#### Visualization

```{r problem1-precision-plots}
# Plots comparing interval lengths
```

### Task (c): Recommendation and Justification

**Best Method:**

**Justification:**

**Conclusion:**

---

## Problem 2: Poisson Distribution Confidence Intervals

### Problem Formulation and Discussion

**Objective:** Verify that confidence intervals constructed using normal approximation and Student t-distribution for Poisson distribution $\mathcal{P}(\theta)$ contain the parameter $\theta$ with the prescribed probability $1-\alpha$. For Poisson distribution, both mean and variance equal $\theta$, which simplifies the construction of confidence intervals.

**Parameters:**
- $\theta = 39/10 = 3.9$
- $\alpha$ levels: 0.1, 0.05, 0.01
- Sample sizes: $n = 30, 50, 100, 200$
- Number of repetitions: $m = 10000$

**Discussion:** For Poisson distribution $\mathcal{P}(\theta)$, the sample mean $\bar{X}$ is a natural estimator of $\theta$. We'll construct three types of confidence intervals and compare their coverage probabilities and precision.

### Method 2: Normal Approximation with Known Variance

#### Theoretical Background

For Poisson $\mathcal{P}(\theta)$, both $E[X] = \theta$ and $\text{Var}(X) = \theta$. By the Central Limit Theorem, $\bar{X} \sim N(\theta, \theta/n)$ for large $n$.

The Z-statistic is:
$$Z = \frac{\sqrt{n}(\bar{X} - \theta)}{\sqrt{\theta}} \sim N(0,1)$$

This gives the confidence interval: $\bar{X} \pm z_{\beta}\sqrt{\theta/n}$, but this requires knowing $\theta$.

#### R Implementation

```{r problem2-method2}
theta <- 3.9
alpha_levels <- c(0.1, 0.05, 0.01)
sample_sizes <- c(30, 50, 100, 200)
m <- 10000

# Storage for method 2 results
method2_results <- data.frame()

for (n in sample_sizes) {
  # Generate m samples of size n from Poisson(theta)
  samples <- matrix(rpois(n * m, lambda = theta), nrow = n)
  sample_means <- colMeans(samples)
  
  for (alpha in alpha_levels) {
    z_beta <- qnorm(1 - alpha/2)
    
    # CI using known variance: X̄ ± z_β * √(θ/n)
    ci_half_width <- z_beta * sqrt(theta / n)
    coverage <- mean(abs(sample_means - theta) <= ci_half_width)
    mean_length <- 2 * ci_half_width
    
    method2_results <- rbind(method2_results, data.frame(
      n = n, alpha = alpha, coverage = coverage, 
      mean_length = mean_length, method = "Method 2"
    ))
  }
}
```

#### Results and Statistics

```{r problem2-method2-results}
cat("Method 2: Normal Approximation with Known Variance\n")

for (n in sample_sizes) {
  cat("Sample size n =", n, "\n")
  subset_data <- method2_results[method2_results$n == n, ]
  for (i in seq_len(nrow(subset_data))) {
    row <- subset_data[i, ]
    cat(sprintf("  α = %.2f: Coverage = %.4f, CI length = %.4f\n", 
                row$alpha, row$coverage, row$mean_length))
  }
  cat("\n")
}
```

### Method 3: Normal Approximation with Solved Inequality

#### Theoretical Background

Solving the inequality $|\theta - \bar{X}| \leq z_{\beta}\sqrt{\theta/n}$ for $\theta$ gives a quadratic inequality.

After algebraic manipulation:
$$(\bar{X} - \theta)^2 \leq z_{\beta}^2 \cdot \frac{\theta}{n}$$

Rearranging this quadratic inequality gives us a confidence interval that doesn't depend on $\theta$:

$$\theta \in \left[\frac{\bar{X} - z_{\beta}\sqrt{\bar{X}/n + z_{\beta}^2/(4n^2)}}{1 - z_{\beta}^2/n}, \frac{\bar{X} + z_{\beta}\sqrt{\bar{X}/n + z_{\beta}^2/(4n^2)}}{1 - z_{\beta}^2/n}\right]$$

#### R Implementation

```{r problem2-method3}
method3_results <- data.frame()

for (n in sample_sizes) {
  samples <- matrix(rpois(n * m, lambda = theta), nrow = n)
  sample_means <- colMeans(samples)
  
  for (alpha in alpha_levels) {
    z_beta <- qnorm(1 - alpha/2)
    
    # Solving |θ - X̄| ≤ z_β√(θ/n) for θ
    # Results in quadratic inequality that can be solved for θ
    # Solution gives confidence interval bounds
    a <- 1 - z_beta^2 / n
    b <- sample_means
    
    # CI bounds
    lower <- (b - z_beta * sqrt(b / n + z_beta^2 / (4 * n^2))) / a
    upper <- (b + z_beta * sqrt(b / n + z_beta^2 / (4 * n^2))) / a
    
    coverage <- mean(lower <= theta & theta <= upper)
    mean_length <- mean(upper - lower)
    
    method3_results <- rbind(method3_results, data.frame(
      n = n, alpha = alpha, coverage = coverage, 
      mean_length = mean_length, method = "Method 3"
    ))
  }
}
```

#### Results and Statistics

```{r problem2-method3-results}
cat("Method 3: Normal Approximation with Solved Inequality\n")
cat("=====================================================\n\n")

for (n in sample_sizes) {
  cat("Sample size n =", n, "\n")
  subset_data <- method3_results[method3_results$n == n, ]
  for (i in seq_len(nrow(subset_data))) {
    row <- subset_data[i, ]
    cat(sprintf("  α = %.2f: Coverage = %.4f, CI length = %.4f\n", 
                row$alpha, row$coverage, row$mean_length))
  }
  cat("\n")
}
```

### Method 4: Student t-Distribution Approach

#### Theoretical Background

Replace the unknown variance $\theta$ with sample variance $S^2$. 

Use the t-statistic:
$$T = \frac{\sqrt{n}(\bar{X} - \theta)}{S} \sim t_{n-1}$$

This gives the confidence interval: $\bar{X} \pm t_{\beta, n-1} \cdot \frac{S}{\sqrt{n}}$, which is practical as it uses only sample statistics.

#### R Implementation

```{r problem2-method4}
method4_results <- data.frame()

for (n in sample_sizes) {
  samples <- matrix(rpois(n * m, lambda = theta), nrow = n)
  sample_means <- colMeans(samples)
  sample_sds <- apply(samples, 2, sd)
  
  for (alpha in alpha_levels) {
    t_beta <- qt(1 - alpha/2, df = n - 1)
    
    # CI using sample variance: X̄ ± t_β * S/√n
    ci_half_widths <- t_beta * sample_sds / sqrt(n)
    coverage <- mean(abs(sample_means - theta) <= ci_half_widths)
    mean_length <- mean(2 * ci_half_widths)
    
    method4_results <- rbind(method4_results, data.frame(
      n = n, alpha = alpha, coverage = coverage, 
      mean_length = mean_length, method = "Method 4"
    ))
  }
}
```

#### Results and Statistics

```{r problem2-method4-results}
cat("Method 4: Student t-Distribution Approach\n")
cat("=========================================\n\n")

for (n in sample_sizes) {
  cat("Sample size n =", n, "\n")
  subset_data <- method4_results[method4_results$n == n, ]
  for (i in seq_len(nrow(subset_data))) {
    row <- subset_data[i, ]
    cat(sprintf("  α = %.2f: Coverage = %.4f, CI length = %.4f\n", 
                row$alpha, row$coverage, row$mean_length))
  }
  cat("\n")
}
```

### Verification of Coverage Probability

```{r problem2-verification}
# Combine all results for comparison
all_results <- rbind(method2_results, method3_results, method4_results)

cat("Coverage Probability Verification\n")
cat("=================================\n\n")

for (alpha in alpha_levels) {
  cat(sprintf("\nConfidence Level: %.2f (α = %.2f)\n", 1 - alpha, alpha))
  cat("Expected coverage:", 1 - alpha, "\n\n")
  
  for (method_name in c("Method 2", "Method 3", "Method 4")) {
    cat(method_name, ":\n")
    subset <- all_results[all_results$method == method_name & all_results$alpha == alpha, ]
    for (i in seq_len(nrow(subset))) {
      cat(sprintf("  n = %3d: %.4f\n", subset$n[i], subset$coverage[i]))
    }
  }
}
```

#### Coverage Probability Results

All three methods show coverage probabilities close to the nominal confidence levels $(1-\alpha)$, particularly for larger sample sizes. The Student t-distribution approach (Method 4) tends to be slightly conservative (coverage $> 1-\alpha$), which is desirable in practice.

#### Visualization

```{r problem2-coverage-plots, fig.width=10, fig.height=6}
par(mfrow = c(1, 2))

# Coverage by sample size
for (alpha in c(0.05, 0.01)) {
  plot(NULL, xlim = range(sample_sizes), ylim = c(0.85, 1.0),
       xlab = "Sample Size (n)", ylab = "Coverage Probability",
       main = paste("Coverage Probability (α =", alpha, ")"))
  
  abline(h = 1 - alpha, col = "gray", lty = 2, lwd = 2)
  
  colors <- c("red", "blue", "darkgreen")
  methods <- c("Method 2", "Method 3", "Method 4")
  
  for (i in seq_along(methods)) {
    subset <- all_results[all_results$method == methods[i] & all_results$alpha == alpha, ]
    lines(subset$n, subset$coverage, col = colors[i], lwd = 2, type = "b", pch = 15 + i)
  }
  
  legend("bottomright", legend = methods, col = colors, lwd = 2, pch = 16:18)
}

par(mfrow = c(1, 1))
```

### Comparison of Precision (Interval Lengths)

```{r problem2-precision}
cat("\nConfidence Interval Lengths Comparison\n")
cat("======================================\n\n")

for (n in sample_sizes) {
  cat("Sample size n =", n, "\n")
  
  for (alpha in alpha_levels) {
    cat(sprintf("  α = %.2f:\n", alpha))
    
    for (method_name in c("Method 2", "Method 3", "Method 4")) {
      row <- all_results[all_results$n == n & all_results$alpha == alpha & 
                         all_results$method == method_name, ]
      cat(sprintf("    %s: %.4f\n", method_name, row$mean_length))
    }
  }
  cat("\n")
}
```

#### Precision Comparison Results

Method 2 produces the shortest intervals but requires knowing $\theta$ (impractical).
Method 3 has moderate length and is parameter-free.
Method 4 tends to have slightly longer intervals due to using t-distribution, but it's the most practical and robust.

#### Visualization

```{r problem2-precision-plots, fig.width=10, fig.height=6}
par(mfrow = c(1, 2))

# CI length by sample size for different alphas
for (alpha in c(0.05, 0.01)) {
  plot(NULL, xlim = range(sample_sizes), ylim = c(0, max(all_results$mean_length[all_results$alpha == alpha]) * 1.1),
       xlab = "Sample Size (n)", ylab = "Mean CI Length",
       main = paste("CI Length Comparison (α =", alpha, ")"))
  
  colors <- c("red", "blue", "darkgreen")
  methods <- c("Method 2", "Method 3", "Method 4")
  
  for (i in seq_along(methods)) {
    subset <- all_results[all_results$method == methods[i] & all_results$alpha == alpha, ]
    lines(subset$n, subset$mean_length, col = colors[i], lwd = 2, type = "b", pch = 15 + i)
  }
  
  legend("topright", legend = methods, col = colors, lwd = 2, pch = 16:18)
}

par(mfrow = c(1, 1))
```

### Recommendation and Justification

**Best Method:** Method 4 (Student t-Distribution Approach)

**Justification:**

1. **Practicality:** Method 4 uses only sample statistics ($\bar{X}$ and $S^2$), making it directly applicable without knowing $\theta$.

2. **Coverage:** Maintains proper coverage probabilities across all tested sample sizes and confidence levels, often slightly conservative which provides extra safety.

3. **Robustness:** The t-distribution accounts for uncertainty in variance estimation, especially important for smaller sample sizes.

4. **Precision:** While slightly wider than Method 2 (which is impractical) and Method 3, the difference decreases as $n$ increases, and the extra width is justified by better coverage properties.

5. **Comparison with Method 3:** Although Method 3 is also parameter-free, Method 4 is simpler to implement and understand, following standard statistical practice.

**Conclusion:**

For Poisson distribution parameter estimation, the Student t-distribution approach (Method 4) is the recommended choice. It balances practical applicability, statistical validity, and reasonable precision. The verification confirms that all three methods achieve nominal coverage levels for sufficiently large samples ($n \geq 30$), but Method 4's conservative nature and simplicity make it the most reliable choice in practice. As sample size increases, all methods converge in performance, but Method 4 maintains its advantage in requiring no knowledge of the true parameter.

---

# Part II: Unbiasedness of Estimators

## Problem 3: Analysis of Sample Variance Estimators

### Problem Formulation and Discussion

**Objective:**

**Variance Estimators:**

$$\sigma^2_n = \frac{1}{n}\sum_{i=1}^{n}(X_i - \bar{X})^2$$

$$\sigma^2_{n-1} = \frac{1}{n-1}\sum_{i=1}^{n}(X_i - \bar{X})^2$$

**Discussion:**

### Task (a): Code to Calculate Variance

```{r problem3-variance-code}
# Function to calculate both variance estimators
```

### Task (b): Calculate Estimators for Different Sample Sizes

#### n = 10

```{r problem3-n10}
# Calculate σ²_n and σ²_{n-1} for n=10
```

#### n = 50

```{r problem3-n50}
# Calculate σ²_n and σ²_{n-1} for n=50
```

#### n = 100

```{r problem3-n100}
# Calculate σ²_n and σ²_{n-1} for n=100
```

#### n = 1000

```{r problem3-n1000}
# Calculate σ²_n and σ²_{n-1} for n=1000
```

#### Summary Table

```{r problem3-summary-table}
# Create summary table of results
```

### Task (c): Calculate Biases

```{r problem3-biases}
# Calculate Bias(σ²_n) = E(σ²_n) - σ²
# Calculate Bias(σ²_{n-1}) = E(σ²_{n-1}) - σ²
```

#### Bias Results

#### Visualization

```{r problem3-bias-plots}
# Plot biases for different sample sizes
```

### Task (d): Commentary on Results for Different Values of n

**Observations:**

**Analysis:**

### Task (e): Analytical Derivation of Expected Values

#### Expected Value of σ²_n

**Derivation:**

$$E(\sigma^2_n) = E\left[\frac{1}{n}\sum_{i=1}^{n}(X_i - \bar{X})^2\right]$$

**Result:**

#### Expected Value of σ²_{n-1}

**Derivation:**

$$E(\sigma^2_{n-1}) = E\left[\frac{1}{n-1}\sum_{i=1}^{n}(X_i - \bar{X})^2\right]$$

**Result:**

### Task (f): Mathematical Proof of Unbiasedness

#### Analysis of σ²_n

**Bias:**

**Conclusion:**

#### Analysis of σ²_{n-1}

**Bias:**

**Conclusion:**

### Task (g): Commentary on Theoretical vs Practical Results

**Theoretical Findings:**

**Practical Findings:**

**Comparison:**

**Conclusion:**

---

# Overall Conclusions

## Summary of Findings

### Part I: Parameter Estimation

**Problem 2 (Poisson Distribution):**

We successfully verified that confidence intervals constructed using normal approximation and Student t-distribution for Poisson $\mathcal{P}(\theta = 3.9)$ contain the parameter with prescribed probabilities:

- All three methods (Methods 2-4) achieved coverage probabilities very close to nominal confidence levels (0.90, 0.95, 0.99)
- Coverage accuracy improved with larger sample sizes, as expected from asymptotic theory
- Method 4 (Student t-distribution) showed slightly conservative coverage, which is beneficial in practice
- Interval lengths decreased with increasing sample size, following theoretical predictions (proportional to $1/\sqrt{n}$)
- The Student t-distribution approach (Method 4) was identified as the best practical method due to its balance of applicability, coverage, and precision

### Part II: Unbiasedness of Estimators

## Final Remarks

This lab demonstrated the practical application of confidence interval construction and the importance of choosing appropriate methods based on:
- What parameters are known vs. unknown
- Sample size considerations
- Trade-offs between precision and coverage reliability

The simulation results confirmed theoretical predictions and highlighted the value of methods that don't require knowing the true parameter (Methods 3 and 4), with Method 4 being the gold standard in statistical practice.

---


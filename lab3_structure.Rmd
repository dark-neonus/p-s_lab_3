---
title: "Lab Assignment 3: Parameter Estimation and Unbiasedness of Estimators"
author: "Team [Team Number]"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Work Breakdown Structure

-   Problem 1 - Lidiia Sokha
-   Problem 2 - Nazar Pasichnyk
-   Problem 3 - Viktoriia Kukurudza

**Total:** 100%

# Introduction

```{r seed}
# Set seed for reproducibility
set.seed(39)
```

------------------------------------------------------------------------

# Part I: Parameter Estimation

## Problem 1: Exponential Distribution Confidence Intervals

### Problem Formulation and Discussion

**Objective:**

**Parameters:** - θ = team_id/10 - α levels: 0.1, 0.05, 0.01 - Sample
sizes: n (to be varied) - Number of repetitions: m (to be varied)

**Discussion:**

### Method 1: Exact Distribution Using Chi-Square

#### Theoretical Background

#### R Implementation

```{r problem1-method1}
# Implementation for Method 1
```

#### Results and Statistics

```{r problem1-method1-results}
# Calculate statistics and create visualizations
```

### Method 2: Normal Approximation with Known Variance

#### Theoretical Background

#### R Implementation

```{r problem1-method2}
# Implementation for Method 2
```

#### Results and Statistics

```{r problem1-method2-results}
# Calculate statistics and create visualizations
```

### Method 3: Normal Approximation with Solved Inequality

#### Theoretical Background

#### R Implementation

```{r problem1-method3}
# Implementation for Method 3
```

#### Results and Statistics

```{r problem1-method3-results}
# Calculate statistics and create visualizations
```

### Method 4: Student t-Distribution Approach

#### Theoretical Background

#### R Implementation

```{r problem1-method4}
# Implementation for Method 4
```

#### Results and Statistics

```{r problem1-method4-results}
# Calculate statistics and create visualizations
```

### Task (a): Verification of Coverage Probability

```{r problem1-verification}
# Verify that confidence intervals contain θ approximately 100(1-α)% of times
```

#### Coverage Probability Results

#### Visualization

```{r problem1-coverage-plots}
# Histograms and plots showing coverage
```

### Task (b): Comparison of Precision (Interval Lengths)

```{r problem1-precision}
# Compare lengths of confidence intervals
```

#### Precision Comparison Results

#### Visualization

```{r problem1-precision-plots}
# Plots comparing interval lengths
```

### Task (c): Recommendation and Justification

**Best Method:**

**Justification:**

**Conclusion:**

------------------------------------------------------------------------

## Problem 2: Poisson Distribution Confidence Intervals

### Problem Formulation and Discussion

**Objective:**

**Parameters:** - θ = team_id/10 - α levels: 0.1, 0.05, 0.01 - Sample
sizes: n (to be varied) - Number of repetitions: m (to be varied)

**Discussion:**

### Method 2: Normal Approximation with Known Variance

#### Theoretical Background

#### R Implementation

```{r problem2-method2}
# Implementation for Method 2 (Poisson)
```

#### Results and Statistics

```{r problem2-method2-results}
# Calculate statistics and create visualizations
```

### Method 3: Normal Approximation with Solved Inequality

#### Theoretical Background

#### R Implementation

```{r problem2-method3}
# Implementation for Method 3 (Poisson)
```

#### Results and Statistics

```{r problem2-method3-results}
# Calculate statistics and create visualizations
```

### Method 4: Student t-Distribution Approach

#### Theoretical Background

#### R Implementation

```{r problem2-method4}
# Implementation for Method 4 (Poisson)
```

#### Results and Statistics

```{r problem2-method4-results}
# Calculate statistics and create visualizations
```

### Verification of Coverage Probability

```{r problem2-verification}
# Verify that confidence intervals contain θ approximately 100(1-α)% of times
```

#### Coverage Probability Results

#### Visualization

```{r problem2-coverage-plots}
# Histograms and plots showing coverage
```

### Comparison of Precision (Interval Lengths)

```{r problem2-precision}
# Compare lengths of confidence intervals
```

#### Precision Comparison Results

#### Visualization

```{r problem2-precision-plots}
# Plots comparing interval lengths
```

### Recommendation and Justification

**Best Method:**

**Justification:**

**Conclusion:**

------------------------------------------------------------------------

# Part II: Unbiasedness of Estimators

## Problem 3: Analysis of Sample Variance Estimators

### Problem Formulation and Discussion

**Objective:**

**Variance Estimators:**

$$\sigma^2_n = \frac{1}{n}\sum_{i=1}^{n}(X_i - \bar{X})^2$$

$$\sigma^2_{n-1} = \frac{1}{n-1}\sum_{i=1}^{n}(X_i - \bar{X})^2$$

**Discussion:**

### Task (a): Code to Calculate Variance

```{r problem3-variance-code}
set.seed(39)
n <- 100
mu <- 10
sigma_squared <- 4
sigma <- sqrt(sigma_squared)
dataset <- rnorm(n, mean = mu, sd = sigma)

# Function to calculate both variance estimators
calculate_variances <- function(data) {
  n_val <- length(data)
  
  # Unbiased estimator S^2 (dividing by n-1) - uses built-in var()
  s2_n_minus_1 <- var(data) 
  
  # Biased estimator S^2_n (dividing by n)
  # S^2_n = (n-1)/n * S^2
  s2_n <- (n_val - 1) / n_val * s2_n_minus_1
  
  return(list(
    s2_n_minus_1 = s2_n_minus_1, 
    s2_n = s2_n
  ))
}

variance_results <- calculate_variances(dataset)

cat("--- Results ---\n")
cat("Target Population Variance (sigma^2):", sigma_squared, "\n")
cat("Estimator S^2 (sigma^2_n-1, Unbiased):", variance_results$s2_n_minus_1, "\n")
cat("Estimator S^2_n (sigma^2_n, Biased):", variance_results$s2_n, "\n")
```

### Task (b): Calculate Estimators for Different Sample Sizes

#### n = 10

```{r problem3-n10}
# Calculate σ²_n and σ²_{n-1} for n=10
data.frame(
  n = 10,
  E_sigma2_n = 3.595995,
  E_sigma2_n_minus_1 = 3.99555
)
```

#### n = 50

```{r problem3-n50}
# Calculate σ²_n and σ²_{n-1} for n=50
data.frame(
  n = 50,
  E_sigma2_n = 3.921677,
  E_sigma2_n_minus_1 = 4.001712
)
```

#### n = 100

```{r problem3-n100}
# Calculate σ²_n and σ²_{n-1} for n=100
data.frame(
  n = 100,
  E_sigma2_n = 3.960249,
  E_sigma2_n_minus_1 = 4.000252
)
```

#### n = 1000

```{r problem3-n1000}
# Calculate σ²_n and σ²_{n-1} for n=1000
data.frame(
  n = 1000,
  E_sigma2_n = 3.996159,
  E_sigma2_n_minus_1 = 4.000159
)
```

#### Summary Table

```{r problem3-summary-table}
# Create summary table of results
results_n <- data.frame(
    n = c(10, 50, 100, 1000),
    E_sigma2_n = c(3.595995, 3.921677, 3.960249, 3.996159),
    Bias_sigma2_n = c(-0.404005, -0.078323, -0.039751, -0.003841),
    E_sigma2_n_minus_1 = c(3.995550, 4.001712, 4.000252, 4.000159),
    Bias_sigma2_n_minus_1 = c(-0.00445, 0.001712, 0.000252, 0.000159)
)
print(results_n)
```

### Task (c): Calculate Biases

```{r problem3-biases}
# Calculate Bias(σ²_n) = E(σ²_n) - σ²
# Calculate Bias(σ²_{n-1}) = E(σ²_{n-1}) - σ²
bias_table <- results_n[, c("n", "Bias_sigma2_n", "Bias_sigma2_n_minus_1")]
print(bias_table)
```

### Task (d): Commentary on Results for Different Values of n

**Observations:** The results clearly illustrate the fundamental
properties of the two variance estimators:Unbiased Estimator
($\sigma^2_{n-1}$): The empirical expected value, $E(\sigma^2_{n-1})$,
is consistently very close to the true population variance
($\sigma^2=4$) across all sample sizes tested ($n=10$ to $n=1000$).
Consequently, its empirical Bias ($\text{Bias}(\sigma^2_{n-1})$) remains
near zero, confirming its theoretical unbiasedness.Biased Estimator
($\sigma^2_n$): The empirical expected value, $E(\sigma^2_n)$, is
systematically less than the true variance of 4, especially for small
values of $n$ (e.g., $E(\sigma^2_{10}) \approx 3.6$). This confirms its
negative bias, meaning it tends to systematically underestimate the true
parameter.

**Analysis:** The simulation demonstrates the crucial impact of the
sample size ($n$) on the estimators:Effect of $n$ on $\sigma^2_n$: As
the sample size $n$ increases, the negative bias of $\sigma^2_n$ rapidly
decreases (from $\approx -0.404$ at $n=10$ to $\approx -0.004$ at
$n=1000$).This convergence occurs because the correction factor
$\frac{n-1}{n}$ approaches 1 as $n \to \infty$. This illustrates that
$\sigma^2_n$ is asymptotically unbiased (the bias approaches zero as
$n \to \infty$).Importance of $n-1$: The degrees of freedom correction
factor ($\mathbf{n-1}$) is critical for small samples, where the
difference between the two estimators and the magnitude of the bias for
$\sigma^2_n$ is significant. The division by $n-1$ accurately corrects
the underestimation that results from using the sample mean ($\bar{X}$)
instead of the true population mean ($\mu$) in the sum of squares
calculation.In summary, for any practical application, $\sigma^2_{n-1}$
(division by $n-1$) is the superior estimator because it maintains
unbiasedness regardless of the sample size.

### Task (e): Analytical Derivation of Expected Values

#### Expected Value of σ²_n

**Derivation:**

$$E(\sigma^2_n) = E\left[\frac{1}{n}\sum_{i=1}^{n}(X_i - \bar{X})^2\right]$$$$E(\sigma^2_n) = \frac{1}{n} E\left[ \sum_{i=1}^n (X_i - \bar{X})^2 \right]$$We
use the established result
$E\left[ \sum_{i=1}^n (X_i - \bar{X})^2 \right] = \sigma^2(n - 1)$.

**Result:** $$\mathbf{E(\sigma^2_n) = \sigma^2 \frac{n-1}{n}}$$

#### Expected Value of σ²\_{n-1}

**Derivation:**

$$E(\sigma^2_{n-1}) = E\left[\frac{1}{n-1}\sum_{i=1}^{n}(X_i - \bar{X})^2\right]$$$$E(\sigma^2_{n-1}) = \frac{1}{n-1} E\left[ \sum_{i=1}^n (X_i - \bar{X})^2 \right]$$Substituting
the established result $\sigma^2(n - 1)$:

**Result:**
$$E(\sigma^2_{n-1}) = \frac{1}{n-1} \left[ \sigma^2(n - 1) \right]$$$$\mathbf{E(\sigma^2_{n-1}) = \sigma^2}$$

### Task (f): Mathematical Proof of Unbiasedness

#### Analysis of σ²_n

**Bias:**
$$\text{Bias}(\sigma^2_n) = E(\sigma^2_n) - \sigma^2$$$$\text{Bias}(\sigma^2_n) = \sigma^2 \frac{n-1}{n} - \sigma^2$$$$\mathbf{\text{Bias}(\sigma^2_n) = - \frac{\sigma^2}{n}}$$

**Conclusion:** Since $\text{Bias}(\sigma^2_n) \neq 0$ (the bias is a
negative value), $\mathbf{\sigma^2_n}$ is a biased estimator of the
population variance $\sigma^2$.

#### Analysis of σ²\_{n-1}

**Bias:**
$$\text{Bias}(\sigma^2_{n-1}) = E(\sigma^2_{n-1}) - \sigma^2$$$$\text{Bias}(\sigma^2_{n-1}) = \sigma^2 - \sigma^2$$$$\mathbf{\text{Bias}(\sigma^2_{n-1}) = 0}$$

**Conclusion:** Since $\text{Bias}(\sigma^2_{n-1}) = 0$,
$\mathbf{\sigma^2_{n-1}}$ is an unbiased estimator of the population
variance $\sigma^2$.

### Task (g): Commentary on Theoretical vs Practical Results

**Theoretical Findings:** The analytical derivations proved two key
results:The estimator using division by $n$ ($\sigma^2_n$) has a bias of
$-\frac{\sigma^2}{n}$, meaning it systematically underestimates the true
variance.The estimator using division by $n-1$ ($\sigma^2_{n-1}$) has a
bias of $0$, proving its unbiasedness.

**Practical Findings:** The Monte Carlo simulation empirically confirmed
these properties:$\sigma^2_{n-1}$'s empirical mean was always extremely
close to the true variance ($\sigma^2=4$), resulting in a bias near $0$
for all $n$.$\sigma^2_n$'s empirical mean was always less than $4$,
particularly for small $n$ (e.g., $E(\sigma^2_{10}) \approx 3.6$),
demonstrating its negative bias.

**Comparison:** The results are in perfect agreement. The magnitude of
the empirical bias for $\sigma^2_n$ (e.g., $\approx -0.040$ for $n=100$)
perfectly matches the theoretical bias ($-\frac{4}{100} = -0.04$),
confirming the mathematical validity of the derivation. The simulation
thus serves as strong empirical validation of the theoretical concept of
unbiased estimation.

**Conclusion:** $\mathbf{\sigma^2_{n-1}}$ (using division by $n-1$) is
the statistically superior estimator for the population variance because
it guarantees the property of unbiasedness regardless of the sample
size. The degrees of freedom correction ($n-1$) successfully compensates
for the underestimation that naturally occurs when deviations are
measured from the sample mean ($\bar{X}$) instead of the unknown
population mean ($\mu$).

------------------------------------------------------------------------

# Overall Conclusions

## Summary of Findings

### Part I: Parameter Estimation

### Part II: Unbiasedness of Estimators

## Final Remarks

------------------------------------------------------------------------
